---
title: 'Module 7: An introduction to MixSIAR'
author: "Andrew Parnell, School of Mathematics and Statistics, University College Dublin"
output:
  beamer_presentation: default
  ioslides_presentation:
    logo: http://www.ucd.ie/handball/images/ucd_brandmark_colour.gif
    transition: slower
    widescreen: yes
---

## Learning outcomes
- Random effects models
- Understand how MixSIAR extends MixSIR and SIAR
- Understand the differences in likelihoods and priors
- Be able to understand output from MixSIAR

## Revision: SIAR model in JAGS

- Let's move back to the proper SIAR model defined yesterday:
$$ y_{ij} \sim N \left( \frac{\sum_{k=1}^K p_k q_{jk} (\mu_{s,jk}+\mu_{c,jk})}{\sum_{k=1}^K p_k q_{jk}}, \frac{\sum_{k=1}^K p_k^2 q_{jk}^2 (\sigma_{s,jk}^2+\sigma_{c,jk}^2)}{(\sum_{k=1}^K p_k q_{jk})^2} +  \sigma_j^2\right) $$
- We also have prior distributions (usually uniform) on $\sigma^2$ and a Dirichlet prior on $p$
- How is this model still a simplification of reality?

## Expanding the SIAR model further

Some (of the many) possible extensions:

1. We are assuming that all consumers have identical dietary proportions
2. We are assuming that residuals, sources and TEFs are uncorrelated across isotopes
3. We are assuming that concentration dependence is known
4. We cannot add in any extra covariates (height, weight, etc, etc)

## Mixed effects models in linear regression

- Often data are available in _groups_, for example wolves might belong to different packs
- We want to capture the different levels of variation, both _within_ groups, and _between_ groups
- Example: suppose $y_{ij}$ is a measurement for individual $i$ in group $j$, $i=1,\ldots,N_j$, $j=1,\ldots,M$
- We might use a model such as:
$$y_{ij} \sim N(\mu + b_j, \sigma^2),\; b_j \sim N(0,\sigma_b^2)$$
- Now $b_j$ is called a _random effect_ and measures the change in the mean for each group
- $\sigma$ measures the standard deviation _within_ a group whilst $\sigma_b$ measures the standard deviation _between_ groups

## Fitting a random effects model in JAGS

```{r,include=FALSE}
library(rjags)
#set.seed(123)
#sigma_b = 0.7
#sigma = 0.5
#M = 3
#N = 23
#group = c(rep(1,10),rep(2,8),rep(3,5))
#b = rnorm(M,0,sigma_b)
#mu = 3
#y = rnorm(N,mu+b[group],sigma)
#dput(round(y,2))
#dput(group)
```

```{r,results='hide'}
modelstring ='
model {
  for(i in 1:N) { y[i] ~ dnorm(mu+b[group[i]],1/pow(sigma,2)) }
  for(j in 1:M) { b[j] ~ dnorm(0,1/pow(sigma_b,2)) }
  mu ~ dnorm(0,0.001)
  sigma ~ dunif(0,100)
  sigma_b ~ dunif(0,10)
}'
data=list(y=c(3.03, 2.68, 2.04, 3.23, 2.82, 2.46, 3.06, 3.05, 
              3.02, 2.95, 3.12, 2.81, 2.69, 2.65, 2.49, 2.73, 
              2.21, 3.92, 4.7, 3.53, 3.89, 3.86, 4.48),
          group=c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 
                  2, 2, 2, 3, 3, 3, 3, 3),
          N=23,M=3)
model=jags.model(textConnection(modelstring), data=data)
output=coda.samples(model=model,
                    variable.names=c("mu","sigma","sigma_b"),
                    n.iter=1000)
```

## Output from the random effects model

```{r,include=FALSE}
modelstring ='
model {
  for(i in 1:N) { y[i] ~ dnorm(b[group[i]],1/pow(sigma,2)) }
  for(j in 1:M) { b[j] ~ dnorm(mu,1/pow(sigma_b,2)) }
  mu ~ dnorm(0,0.001)
  sigma ~ dunif(0,100)
  sigma_b ~ dunif(0,10)
}'
data=list(y=c(3.03, 2.68, 2.04, 3.23, 2.82, 2.46, 3.06, 3.05, 
              3.02, 2.95, 3.12, 2.81, 2.69, 2.65, 2.49, 2.73, 
              2.21, 3.92, 4.7, 3.53, 3.89, 3.86, 4.48),
          group=c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 
                  2, 2, 2, 3, 3, 3, 3, 3),
          N=23,M=3)
model=jags.model(textConnection(modelstring), data=data)
output=coda.samples(model=model,
                    variable.names=c("mu","sigma","sigma_b"),
                    n.iter=1000)
```

```{r}
t(round(apply(output[[1]],2,quantile,probs=c(0.025,0.5,0.975)),2))
```
- You need a reasonable number of groups to estimate $\sigma_b$ well. When the number of groups is very small you might run into problems
- You could also include `b` in the `variable.names` if you want the deviations from the overall mean
- Convergence is often better if you _hierarchically centre_ the model, which means set $b_j \sim N(\mu,\sigma_b^2)$ in the previous slides' JAGS code (if you look in the code for these slides that is what I ran)

## Defining mixed effects models in the SIMM case

- In the SIMM case we might have information that some consumers might share the same dietary proportions
- We might be interested in how the dietary proportions vary between groups and within groups, just as in the simple example
- An issue is: how do we achieve this in the SIMM case?
- One solution was elegantly proposed by Semmens _et al_ in their PLoS ONE 2009 paper

## A hierchical SIMM

- Semmens _et al_ differentiate between the _overall_ or _mean_ dietary proportions $p_{mean}$, and the _deviations_ $p_{dev,j}$ from this mean for each _group_ $p_j$ 
- They use the relationship:
$$ p_j = w p_{mean} + (1-w) p_{dev,j} $$
where $w$ is a mixing constant which determines how much the group behaves like the mean (high $w$), or not (low $w$)
- They give the mean and the deviation dietary proportions Dirichlet($\alpha_1,\ldots,\alpha_K$) prior distributions
- They give $w$ a standard $U(0,1)$ prior

## More on  random effects in proportions

- A slightly more flexible prior distribution for proportions is obtained by transforming the proportions instead
- We already met this with logistic regression where we can use:
$$f = logit(p) = \log \left( \frac{p}{1-p} \right) \mbox{ or equivalently } p = \frac{\exp(f)}{\exp(f)+1}$$
- When we have multiple proportions a generalisation of this is the _centralised log ratio_ (CLR) or _softmax_ transformation:
$$\left[ p_1,\ldots,p_K \right] = \left[ \frac{\exp(f_1)}{\sum_j \exp (f_j)}, \ldots, \frac{\exp(f_K)}{\sum_k \exp (f_k)} \right]$$

## The CLR transformation

- In logistic regression we can put a prior distribution on $f$ (i.e. $logit(p)$) e.g. $f \sim N(\alpha + \beta x, \sigma^2)$ which allows us to relate the probability $p$ to a covariate $x$. We can use the normal distribution because $f$ is unrestricted
- In CLR regression, we put a prior on the $f_k$ so that each one relates to the covariate with different coefficient values
- The CLR transformation guarantees that all the dietary proportions will sum to 1
- More on this tomorrow

## Random effects for individuals

- We don't necessarily need a grouping structure (e.g. pack, sex, etc) to be able to include random effects in a SIMM
- In a SIMM we might reasonably assume that every consumer is eating something slightly different and want to quantify the overall mean diet as well as the variability between consumers
- We can do this by modelling each consumer's dietary proportion $p_{ik}$ with a normally distributed prior on the CLR transform of $p$

## A 'simple' CLR example {.smaller}
```{r,eval=FALSE}
modelstring ='
model {
  for (j in 1:J) {
    for (i in 1:N) { 
      y[i,j] ~ dnorm(inprod(p[i,]*q[,j], s_mean[,j]+c_mean[,j]) / inprod(p[i,],q[,j]), 1/var_y[i,j])
      var_y[j] <- inprod(pow(p[i,]*q[,j],2),1/s_prec[,j]+1/c_prec[,j])/pow(inprod(p[i,],q[,j]),2) + pow(sigma[j],2)
    }
  }
  for(i in 1:N) {
    p[i,1:K] <- expf[i,]/sum(expf[i,])
    for(k in 1:K) {
      expf[i,k] <- exp(f[i,k])
      f[i,k] ~ dnorm(mu_f[k],1/pow(sigma_f[k],2))
    }
  }
  for(k in 1:K) { 
    mu_f[k] ~ dnorm(0,1)
    sigma_f[k] ~ dgamma(2,1)
  }
  for(j in 1:J) { sigma[j] ~ dunif(0,10) }
}
'
```


```{r,include=FALSE}
modelstring ='
model {
  for (i in 1:N) {
    for (j in 1:J) { 
      y[i,j] ~ dnorm(inprod(p[i,]*q[,j], s_mean[,j]+c_mean[,j]) / inprod(p[i,],q[,j]), 1/var_y[i,j])
      var_y[i,j] <- inprod(pow(p[i,]*q[,j],2),1/s_prec[,j]+1/c_prec[,j])/pow(inprod(p[i,],q[,j]),2) 
        + pow(sigma[j],2)
    }
  }
  for(i in 1:N) {
    p[i,1:K] <- expf[i,]/sum(expf[i,])
    for(k in 1:K) {
      expf[i,k] <- exp(f[i,k])
      f[i,k] ~ dnorm(mu_f[k],1/pow(sigma_f[k],2))
    }
  }
  for(k in 1:K) { 
    mu_f[k] ~ dnorm(0,1)
    sigma_f[k] ~ dgamma(2,1)
  }
  for(j in 1:J) { sigma[j] ~ dunif(0,10) }
}
'
```

## CLR model: R code
```{r,include=FALSE}
library(siar)
data(geese1demo,sourcesdemo,correctionsdemo,concdepdemo)
con = geese1demo
sources = as.matrix(sourcesdemo[,2:5])
tefs = as.matrix(correctionsdemo[,2:5])
cd = as.matrix(concdepdemo[,c(2,4)])
```
```{r,results='hide'}
data=list(y=con,s_mean=sources[,c(1,3)],s_prec=1/sources[,c(2,4)]^2,
          c_mean=tefs[,c(1,3)],c_prec=1/tefs[,c(2,4)]^2,
          q=cd,N=nrow(con),K=nrow(sources),
          J=ncol(con))
model=jags.model(textConnection(modelstring), data=data)
output=coda.samples(model=model,variable.names=c('p','sigma','mu_f','sigma_f'),
                    n.iter=10000)
out_summ = summary(output)
```

## Output
```{r}
head(out_summ$statistics,12)
```

## Notes about the CLR model

- This is a great starter script for your own work. If you can understand this code and adapt it to your data you can get some really powerful results
- We can now put covariates in the model: we just have to expand $\mu_f$ in the previous JAGS code
- We now have individual dietary proportion estimates ($p_{ik}$) and overall dietary proportion estimates (via CLR transform of $\mu_k)$, and also estimates of the variability (from $\sigma_f$)
- Things start to get complicated with prior distributions at this stage. Be very careful and always examine prior sensitivity by re-running the model with slightly different prior distributions. Look at the effect on $p$ (the effect on $\mu_f$ and $\sigma_f$ is less important)

## Back to MixSIAR

- MixSIAR implements a version of the CLR jags code already shown, and through this allows the introduction of random effects and covariates
- It's a bit limited in that:

    1. It's recommended to work with a GUI
    2. You can only put a certain number of random effects/covariates into the model
    3. It doesn't currently take account of uncertainty in TEFs (it calls this _discrimination_)
    4. It doesn't allow for much choice in prior distributions

- However, it's a great start at finding simple ways to run more complicated models

## The main MixSIAR input screen

<center><img src="mixsiar_screen.tiff" height="500px" /></center>

## Running the Wolves example

- In the MixSIAR folder there are three files: `wolves_consumer.csv`, `wolves_dicrimination.csv`, and `wolves_sources.csv`
- These contain the consumer isotope data with two covariates, the discrimination data (no standard deviations), and the source data (for each different region)
- Note that MixSIAR allows you to input either the sources means/sds (as we have been using), or the raw source data, to which it fits a model to estimate the source parameters
- These data are similar to those used in the Semmens _et al_ PLoS ONE paper already mentioned. The methods are more fully described in that paper
- The variables pack and region here are included as _nested_ random effects, meaning that there are two layers of random effects which measure variability between regions and variability between packs. The overall residual measures variability within pack

## Wolves example - consumers

<center><img src="mixsiar_screen_mix.tiff" height="500px" /></center>

## Wolves example - sources

<center><img src="mixsiar_screen_source.tiff" height="500px" /></center>

## Wolves example - isospace plot

<center><img src="mixsiar_screen_isospace.tiff" height="500px" /></center>

## Wolves example - running

- First run the model as a test go, then choose a normal run
- These models are starting to get complicated, they take a while to run 
- MixSIAR write JAGS code on the fly. Check out the JAGS model structure in `MixSIAR_model.txt`
- When it's finished click on `process output` to get at the (many) results
- You can also access an R script version of this (much better) in `mixsiar_script.r`

## Analysing output

- First, check convergence using Brooks-Gelman-Rubin or Geweke (both previously covered)
- You then have access to histograms of the posterior distributions of the overall means, and also for each level of the hierarchy; here pack and region
- They also produce a pairs plot (like the matrix plot of yesterday) which can tell you about model inadequacy
- Use the `mixsiar_script.r` if you want full access to the JAGS output for any further analysis

## MixSIAR output - trace plot

<center><img src="trace.tiff" height="500px" /></center>

## MixSIAR output - random effect densities

<center><img src="re_sds.tiff" height="500px" /></center>

## MixSIAR output - overall proportion densities

<center><img src="overall_p.tiff" height="500px" /></center>

## MixSIAR output - matrix plot

<center><img src="matrix_plot.tiff" height="500px" /></center>

## Summary

- We have looked at the differences between SIAR and MixSIAR
- This has included: random effects, the centralised log ratio
- We have run one of the more complicated MixSIAR examples
