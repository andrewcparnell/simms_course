---
title: "Differences between regression models and SIMMs"
author: Andrew Parnell \newline \texttt{andrew.parnell@mu.ie}   \newline \vspace{1cm}
  \newline \includegraphics[width=3cm]{../maynooth_uni_logo.jpg}
output:
  beamer_presentation:
    includes:
      in_header: ../header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
```


## Learning outcomes

- Be able to describe the differences and similarities between a regression model and a SIMM
- Understand the likelihood and prior distribution in a basic SIMM
- Know how to check convergence and model performance in a Bayesian model

## Revision: linear regression

- In many statistical problems we have a _response variable_ $y_i$ observed on individuals $i=1,\ldots,N$
- We also have an _explanatory variable_ $x_i$ from which we want to predict $y_i$
- For example, $y_i$ could be the weight of an animal, and $x_i$ could be the proportion of a certain food source in its diet

The usual linear regression model is written as:
$$y_i = \alpha + \beta x_i + \epsilon_i$$
where $\epsilon_i \sim N(0,\sigma^2)$. Another way of writing this model is:
$$y_i \sim N(\alpha + \beta x_i, \sigma^2)$$

## Example: simple data
\small
```{r, fig.height = 5}
x=c(18.07, 52.59, 54.93, 79.31, 89.58)
y=c(7.89, 12.41, 13.34, 19.3, 19.52)
plot(x,y,
     xlab='Percentage of food source in diet (x)',
     ylab='Weight (y)',
     las=1)
abline(a=4.17,b=0.18,col='red')
```

## Running a linear regression in JAGS

```{r,include=FALSE}
library(rjags)
```
```{r,results='hide'}
modelstring ='
model {
  for(i in 1:N) { 
    y[i] ~ dnorm(alpha + beta*x[i],sigma^-2) 
  }
  alpha ~ dnorm(0,100^-2) # Note: vague priors
  beta ~ dnorm(0,100^-2)
  sigma ~ dunif(0,100)
}
'
data=list(x=c(18.07, 52.59, 54.93, 79.31, 89.58),
          y=c(7.89, 12.41, 13.34, 19.3, 19.52),
          N=5)
model=jags.model(textConnection(modelstring), data=data)
output=coda.samples(model=model,
                    variable.names=c("alpha","beta"),
                    n.iter=10000)
```

## Output from linear regression

\small

```{r,fig.height = 6}
par(mfrow=c(1,2))
plot(density(output[[1]][,'alpha']),main='Posterior for alpha')
plot(density(output[[1]][,'beta']),main='Posterior for beta')
```

## More output from linear regression

\small

```{r}
summary(output)
```



## How do we choose a likelihood and a prior for this situation?

- When we're in a standard linear regression situation the likelihood is always a normal distribution. When we use other likelihoods we're running a _generalised linear model_
- The prior for the intercept ($\alpha$) and slope ($\beta$) might come from previous experiments, or in this case are set as vague. Similarly for the residual standard deviation
- Sometimes _re-parameterising_ the model will help with setting the priors. For example, it might be easier to re-write the model as $y_i = \alpha + \beta (x_i - \bar{x}) + \epsilon_i$. Now the parameter $\alpha$ represents the mean value of $y$ at the mean value of $x$ (denoted $\bar{x}$). This might be easier to put a prior distribution on

## Example 2: a generalised linear model situation, e.g. Logistic regression

- Suppose now that rather than observing $y$ as the weight of the animal, we have observed $y$ as whether or not the animal was male ($y_i=1$) or female ($y_i=0$)
- The goal of the model is now to estimate the relationship between dietary proportion and the probability of being male. 
- When the response variable is binary we use a GLM called _logistic regression_. We can write this new model as:
$$ y_i \sim Bin(1,p_i),\; logit(p_i) = \alpha + \beta x_i$$
where $logit(p) = \log \left( \frac{p}{1-p} \right)$. Note that $p_i$ directly measures the probability of each individual being male and has to lie between 0 and 1

## Example 2 in JAGS
\small
Note: this isn't a great model as the data set is very small
```{r,eval=FALSE}
modelstring ='
model {
  for(i in 1:N) { 
    y[i] ~ dbin(p[i],1) 
    logit(p[i]) <- alpha + beta*x[i]
  }
  alpha ~ dnorm(0,100^-2)
  beta ~ dnorm(0,100^-2)
}
'
data=list(x=c(18.07, 52.59, 54.93, 79.31, 89.58),
          y=c(0,1,0,1,1),
          N=5)
model=jags.model(textConnection(modelstring), data=data)
output=coda.samples(model=model,
                    variable.names=c("alpha","beta"),
                    n.iter=10000)
```

## Moving on to SIMMs - what do the data look like?

- Let's start with a very simple version:
    - 1 isotope
    - 2 food sources
    - 9 consumers
    - No other complications
- We'll use some of the Geese data from the SIAR package
- Reminder: you can install the up-to-date version of SIAR with:
```{r,eval=FALSE}
library(devtools)
install_github("andrewljackson/siar")
library(siar)
```

## Plotting the data

\small
- Use the second isotope ($\delta^{13}$C) and the first two food sources (Zostera and Grass)
- Create a plot:
```{r,include=FALSE}
library(siar)
```
```{r,eval=FALSE}
# Load in the data
data(geese1demo); data(sourcesdemo)
consumers = geese1demo[,2]
sources = sourcesdemo[1:2,4:5]
con_grid = seq(-35,-5,length=100)
plot(con_grid,dnorm(con_grid,
                    mean=sources[2,1],sd=sources[2,2]),
     type='l',col='red',xlab='d13C',ylab='Probability density')
lines(con_grid,dnorm(con_grid
                     ,mean=sources[1,1],sd=sources[1,2]),
      col='blue')
points(consumers,rep(0,9))
legend('topright',legend=c('Grass','Zostera','Consumers'),
       lty=c(1,1,-1),pch=c(-1,-1,1),col=c('red','blue','black'))
```

## A simple isospace plot
```{r,echo=FALSE}
# Load in the data
data(geese1demo); data(sourcesdemo)
consumers = geese1demo[,2]
sources = sourcesdemo[1:2,4:5]
con_grid = seq(-35,-5,length=100)
plot(con_grid,dnorm(con_grid,mean=sources[2,1],sd=sources[2,2]),
     type='l',col='red',xlab='d13C',ylab='Probability density')
lines(con_grid,dnorm(con_grid,mean=sources[1,1],sd=sources[1,2]),
      col='blue')
points(consumers,rep(0,9))
legend('topright',legend=c('Grass','Zostera','Consumers'),
       lty=c(1,1,-1),pch=c(-1,-1,1),col=c('red','blue','black'))
```



## A first model for this simple SIMM

- Let $y_i$ be the $\delta^{13}$C value for individual $i$, $i=1,\ldots,9$
- Let $s_k$ be the source value for source $k$, $k=1,2$
- Let $p_k$ be the dietary proportion for source $k$

The likelihood can now be written as:
$$ y_i = p_1 \times s_1 + p_2 \times s_2 + \epsilon_i \;\mbox{or}\; y_i \sim N\left(\sum_{k=1}^2 p_ks_k,\sigma^2\right)$$

so just like a regression model with a slightly different mean!

$\epsilon_i \sim N(0,\sigma^2)$ as usual, though including this term is (strangely) controversial

## Prior distributions for the SIMM

- The parameters for this simple model are $s_1,s_2$, $p_1,p_2$, and $\sigma$
- We have external data on the $s_k$ values, so it makes sense to put a prior distribution $s_k \sim N(\mu_{s_k},\sigma_{s_k}^2)$ on each of these
- The dietary proportions must sum to 1, i.e. $p_2 = 1-p_1$ so we have only have 1 parameter to place a prior on. We might use $p_1 \sim U(0,1)$ if no prior knowledge. 
- An alternative is the Beta distribution which can put more weight on lower or higher proportions
- We usually have little information on $\sigma$, but the isospace plot will usually give a rough guide to the likely range of values

## A simple SIMM in JAGS
\small
```{r,results='hide'}
modelstring ='
model {
  for(i in 1:N) { 
    y[i] ~ dnorm(p_1*s_1+p_2*s_2,sigma^-2) 
  }
  p_1 ~ dunif(0,1)
  p_2 <- 1-p_1
  s_1 ~ dnorm(s_1_mean,s_1_sd^-2)
  s_2 ~ dnorm(s_2_mean,s_2_sd^-2)
  sigma ~ dunif(0,10)
}
'
data=list(y=consumers,s_1_mean=sources[1,1],
          s_1_sd=sources[1,2],
          s_2_mean=sources[2,1],s_2_sd=sources[2,2],
          N=length(consumers))
model=jags.model(textConnection(modelstring), data=data)
output=coda.samples(model=model,
                    variable.names=c("p_1","p_2"),
                    n.iter=10000)
```

## Summarising the output

\small

```{r,fig.align='center'}
plot(density(output[[1]][,1]),xlab='Proportion',
     ylab='Probability density',main='Proportion of Zostera')
```

## Model checking and convergence

- How do you know whether the model fits the data well or not?
- How do you know that JAGS fitted the model OK?
- The model fit can be checked by running _cross-validation_ (leaving out chunks of the data and getting the model to predict the $y$ values of the left out data) or _posterior predictive_ checks, amongst many other methods
- The fitting performance can be evaluated via _convergence checking_. This involves looking at the posterior samples and checking that the values are stable
- Another question (which we will look at in a later session) is whether this is the 'best' model for the data

## Model checking

- Adding a posterior predictive check is as simple as adding an extra line to the JAGS code 
```{r,eval=FALSE}
modelstring ='
  ...
  for(i in 1:N) { 
    y[i] ~ dnorm(p_1*s_1+p_2*s_2,sigma^-2) 
    y_pred[i] ~ dnorm(p_1*s_1+p_2*s_2,sigma^-2) 
  }
  ...
'
```
- `y_pred` is included as another parameter, and is thus estimated as part of the model. 
- We then have both the true $y$ values and some estimated $y$ values from the model

## Model checking output

\small

```{r,include=FALSE}
modelstring ='
model {
  for(i in 1:N) { 
    y[i] ~ dnorm(p_1*s_1+p_2*s_2,sigma^-2) 
    y_pred[i] ~ dnorm(p_1*s_1+p_2*s_2,sigma^-2) 
  }
  p_1 ~ dunif(0,1)
  p_2 <- 1-p_1
  s_1 ~ dnorm(s_1_mean,s_1_sd^-2)
  s_2 ~ dnorm(s_2_mean,s_2_sd^-2)
  sigma ~ dunif(0,10)
}
'
model=jags.model(textConnection(modelstring), data=data)
output=coda.samples(model=model,variable.names=c("y_pred"),
                    n.iter=10000)
```
```{r}
y_pred_quantiles = apply(output[[1]],2,'quantile',probs=c(0.05,0.95))
round(cbind(data$y,t(y_pred_quantiles)),2)
```
Just 1 observation outside the 90% CI. Looks to be an OK model.

## Convergence checking 

\small

- When JAGS runs a model it creates initial guesses of the parameter values and then creates many consecutive samples moving away from the initial values towards the true posterior distribution
- Mathematical theory says that the samples must eventually come from the posterior distribution but this may take a very long time!
- Another method for ensuring convergence is to start JAGS with multiple different starting values and see if each model run (known as a _chain_) converges to the same posterior distribution
- You can supply initial guesses and the number of chains to JAGS when you run it:

\vspace{-0.5cm}

```{r,results='hide'}
inits = function() {
  list('p_1'=runif(1),'s_1'=rnorm(1),'s_2'=runif(1),
       'sigma'=runif(1,0,10))
}
model=jags.model(textConnection(modelstring), data=data, 
                 inits=inits, n.chains=3)
output=coda.samples(model=model,variable.names=c("p_1"),
                    n.iter=10000)
```

\vspace{-0.7cm}

- The `output` created will now be a list of length 3, so we can look at the different chains with e.g. `output[[2]][,'p_1']`

## Convergence checking 2

- You can start by plotting the output from JAGS:
```{r,fig.align='center'}
plot(output)
```

## Convergence checking 3

- The different colours show the three chains. The location and variability should be broadly the same between chains
- There are some useful statistical tests for convergence, including the _Geweke_ test which looks to see whether the mean is stable in each half of the iterations, or the _Brooks, Gelman, Rubin_ (BGR) test which looks to see whether different chains match
- The Geweke test only requires one chain and is used by SIAR
- The BGR test is best if you have multiple chains

## Convergence checking 4

- If you started by choosing bad initial values you might want to remove an initial chunk of the samples. This is known as the burn in and can be set using the `n.adapt` command in the `jags.model` function call
- Ideally the samples from the posterior distribution should be independent. If the algorithm isn't working well you can _thin_ them out with the `thin` argument in the `coda.samples` function. The auto-correlation plot produced by `acf` in `R` can tell you whether you need to thin or not
- Finally, we need to choose the number of iterations. For very simple models 1,000 is usually fine, but for very complicated models you can sometimes need hundreds of thousands or millions. 10,000 is usually a good number for most problems. You can set the number of iterations in JAGS with `n.iter`

SIAR has its own commands for dealing with burn-in/thinning/iterations

## Summary

- A SIMM is very similar to a linear regression. Things get slightly more complicated when we move to multiple isotopes
- The priors for a SIMM involve distributions for the source values, the dietary proportions, and the residual standard deviation
- When running a Bayesian model, remember to check your model (if possible) using posterior predictive checks, and convergence checking


