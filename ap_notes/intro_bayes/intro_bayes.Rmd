---
title: "Introduction to Bayesian Statistics"
author: Andrew Parnell \newline \texttt{andrew.parnell@mu.ie}   \newline \vspace{1cm}
  \newline \includegraphics[width=3cm]{../maynooth_uni_logo.jpg}
output:
  beamer_presentation:
    includes:
      in_header: ../header.tex
editor_options: 
  chunk_output_type: console
classoption: "aspectratio=169"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf', fig.height = 5)
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
```

## Learning outcomes

- Know the difference between Frequentist and Bayesian statistics
- Understand the terms posterior, likelihood and prior. Be able to suggest suitable probability distributions for these terms
- Be able to interpret the posterior distribution through plots, summaries, and credible intervals

A bigger aim, either:

1. Stop using SIAR (for dietary proportions) and start writing your own JAGS code
2. Stop using SIAR and start using MixSIAR/simmr instead

## Who was Bayes?

*An essay towards solving a problem on the doctrine of chances* (1763)

$$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$

\begin{center}
\includegraphics[width=4cm]{../Thomas_Bayes.pdf}
\end{center}

## What is Bayesian statistics?

- Bayesian statistics is based on an interpretation of Bayes' theorem
- All quantities are divided up into _data_ (i.e. things which have been observed) and _parameters_ (i.e. things which haven't been observed)
- We use Bayes' interpretation of the theorem to get the _posterior probability distribution_, the probability of the unobserved given the observed
- Used now in almost all areas of statistical application (finance, medicine, environmetrics, gambling, etc, etc)

## Why is this relevant to SIMMs?

- Easy to specify Bayesian models hierarchically in layers so that the data depend on some parameters, which then depend on further parameters, and so on. This allows us to create richer statistical models which will better match reality
- Almost all the modern Stable Isotope Mixing Models (SIMMs) use Bayesian statistics
- MixSIR, SIAR, MixSIAR, simmr, IsotopeR, ...

## What is Bayes' theorem?

Bayes' theorem can be written in words as:

$$\mbox{posterior is proportional to likelihood times prior}$$
... or ...
$$\mbox{posterior} \propto \mbox{likelihood} \times \mbox{prior}$$
  
Each of the three terms _posterior_, _likelihood_, and _prior_ are _probability distributions_ (pdfs).

In a Bayesian model, every item of interest is either data (which we will write as $x$) or parameters (which we will write as $\theta$). Often the parameters are divided up into those of interest, and other _nuisance parameters_

## Bayes' theorem in more detail

Bayes' equation is usually written mathematically as:
$$p(\theta|x) \propto p(x|\theta) \times p(\theta)$$
or, more fully:
$$p(\theta|x) = \frac{p(x|\theta) \times p(\theta)}{p(x)}$$

- The _posterior_ is the probability of the parameters given the data
- The _likelihood_ is the probability of observing the data given the parameters (unknowns)
- The _prior_ represents external knowledge about the parameters


## A very simple linear regression example

Suppose you had some data that looked like this:
```{r, echo=FALSE}
with(stackloss, plot(Air.Flow, stack.loss))
```

## What you are used to doing

\tiny
```{r}
model = lm(stack.loss ~ Air.Flow, data = stackloss)
summary(model)
```
\normalsize

## What you will now get instead

```{r, echo=FALSE, results='hide', message=FALSE}
library(R2jags)
model_code ='
model {
  for(i in 1:N) { 
    y[i] ~ dnorm(intercept + slope*x[i], residual_sd^-2) 
  }
  intercept ~ dnorm(0,100^-2)
  slope ~ dnorm(0,100^-2)
  residual_sd ~ dunif(0,100)
}
'
data=list(x=stackloss$Air.Flow,
          y=stackloss$stack.loss,
          N=nrow(stackloss))
model_parameters =  c("intercept", "slope", "residual_sd")
model_run = jags(data = data,
                 parameters.to.save = model_parameters,
                 model.file = textConnection(model_code))
```
\tiny
```{r}
print(model_run)
```
\normalsize 

## Using prior information

- The Bayesian model in the previous slide divided up everything into _parameters_ (the intercept, slope and residual standard deviation), and data (the x and y values)
- The software in the background created a posterior probability distribution of the parameters given the data
- The model I fitted used vague _prior information_. However, if we had done a previous experiment that suggested the intercept should be around -30 with standard deviation 5 we can put this in the model

## A model with prior information

```{r, echo=FALSE, results='hide', message=FALSE}
model_code2 ='
model {
  for(i in 1:N) { 
    y[i] ~ dnorm(intercept + slope*x[i], residual_sd^-2) 
  }
  intercept ~ dnorm(-30,5^-2)
  slope ~ dnorm(0,100^-2)
  residual_sd ~ dunif(0,100)
}
'
model_run2 = jags(data = data,
                 parameters.to.save = model_parameters,
                 model.file = textConnection(model_code2))
```
\tiny
```{r}
print(model_run2)
```
\normalsize 

## An early example of a Bayesian model

- To create the Bayesian version of this model I used the following JAGS code:

\scriptsize
```{r, eval = FALSE}
model_code ='
model {
  # Likelihood
  for(i in 1:N) { 
    y[i] ~ dnorm(intercept + slope*x[i], residual_sd^-2) 
  }
  # Priors
  intercept ~ dnorm(0,100^-2)
  slope ~ dnorm(0,100^-2)
  residual_sd ~ dunif(0,100)
}
'
```
\normalsize

## How do I specify the prior distribution?

There are several choices when it comes to specifying prior distributions:

- _Informative_, when there is information from a previous study, or other good external source, e.g intercept $\sim N(-30,5^2)$
- _Vague_, when there is only weak information, perhaps as to the likely range of the parameter e.g. intercept $\sim N(0,100^2)$
- _Flat_, when there is no information at all about a parameter (very rare). In JAGS, write `intercept ~ dflat()`

In fact, choosing the prior and choosing the likelihood are very similar problems

## Choosing likelihoods and priors {.smaller}

When creating Bayesian models it's helpful to know a lot of probability distributions. The ones we will use most are:

\small
\begin{tabular}{p{3cm}lp{4cm}}
\hline
Distribution & Range of parameter & Useful for \\
\hline
Normal, $N(\mu,\sigma^2)$ & $(-\infty,\infty$) & A good default choice \\
Uniform, $U(a,b)$ & $(a,b)$ & Vague priors when we only know the range of the parameter \\
Binomial, $Bin(k,\theta)$ & $[0,k]$ & Count or binary data restricted to have an upper value \\
Poisson, $Po(\lambda)$ & $[0,\infty)$ & Count data with no upper limit \\
Gamma, $Ga(\alpha,\beta)$ & $(0,\infty)$ & Continuous data with a lower bound of zero \\
Multivariate Normal, $MVN(\mu,\Sigma)$ & $(-\infty,\infty$) & Multivariate unbounded data with correlation between parameters/observations \\
\hline
\end{tabular}

## Creating the posterior distribution

- It only takes a few lines of `R` code (and a few more lines of JAGS code) to calculate the posterior distribution
- However this processes will be slower and harder when we have lots of parameters, and complicated prior distributions
- Almost always in the Bayesian world we have to resort to _simulation_ rather than maths to get to the posterior distribution
- This means that we obtain _samples_ from the posterior distribution rather than creating the probability distribution directly
- JAGS uses Markov chain Monte Carlo (MCMC) to create these samples. We will talk about this a bit more in later lectures/discussion

## Summarising the posterior distribution

- Because we obtain samples from the posterior distribution, we can create any quantity we like from them
- e.g. we can obtain the mean or standard deviation simply from combining the samples together
- We can create quantiles e.g. 50% for the median
- We can create a Bayesian _credible interval_ (CI) by calculating lower and upper quantiles
- When the posterior distribution is messy (e.g. multi-modal) we can use a _highest posterior density_ (HPD) region

## Example: {.smaller}

```{r,include=FALSE}
model_code ='
model {
  for(i in 1:N) { 
    y[i] ~ dnorm(intercept + slope*x[i], residual_sd^-2) 
  }
  intercept ~ dnorm(0,100^-2)
  slope ~ dnorm(0,100^-2)
  residual_sd ~ dunif(0,100)
}
'
data=list(x = stackloss$Air.Flow,
          y = stackloss$stack.loss,
          N = nrow(stackloss))
model_parameters =  c("intercept", "slope", "residual_sd")
model_run = jags(data = data,
                 parameters.to.save = model_parameters,
                 model.file = textConnection(model_code))
```

From the earlier simple example. First 5 posterior samples of the slope
```{r}
post_slope = model_run$BUGSoutput$sims.list$slope
post_slope[1:5]
```
The mean and standard deviation: 
```{r}
c(mean(post_slope),sd(post_slope))
```
A 95% credible interval
```{r}
quantile(post_slope,probs=c(0.025,0.975))
```

## Why is this better?

The Bayesian approach has numerous advantages:

- It's easier to build complex models and to analyse the parameters you want directly
- We automatically obtain the best parameter estimates and their uncertainty from the posterior samples
- It allows us to get away from (terrible) null hypothesis testing and $p$-values

## Some further reading

- The Bayesian bible: Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). _Bayesian Data Analysis_, Third Edition. CRC Press.
- The MCMC bible: Brooks, S., Gelman, A., Jones, G., & Meng, X. (2011). _Handbook of Markov Chain Monte Carlo_. CRC Press.
- Something simpler: McCarthy, M. A. (2007). _Bayesian Methods for Ecology_. Cambridge University Press.

## Summary

- Bayesian statistical models involve a likelihood and a prior. These both need to be carefully chosen. From these we create a posterior distribution
- The likelihood represents the information about the data generating process, the prior represents information about the unknown parameters
- We usually create and analyse samples from the posterior probability distribution of the unknowns (the parameters) given the knowns (the data)
- From the posterior distribution we can create means, medians, standard deviations, credible intervals, etc

