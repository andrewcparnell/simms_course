---
title: "Introduction to Bayesian Statistics"
author: "Andrew Parnell, School of Mathematics and Statistics, University College Dublin"
output:
  ioslides_presentation:
    logo: ../ucd_brandmark_colour.gif
    transition: slower
    widescreen: yes
  beamer_presentation: default
---

## Learning outcomes

- Know the difference between Frequentist and Bayesian statistics
- Understand the terms posterior, likelihood and prior. Be able to suggest suitable probability distributions for these terms
- Be able to interpret the posterior distribution through plots, summaries, and credible intervals

A bigger aim, either:

1. Stop using SIAR (for dietary proportions) and start writing your own JAGS code
2. Stop using SIAR and start using MixSIAR/simmr instead

## Who was Bayes?

*An essay towards solving a problem on the doctrine of chances* (1763)

$$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$

<center><img src="Thomas_Bayes.gif" width=40%/></center>

## What is Bayesian statistics?

- Bayesian statistics is based on an interpretation of Bayes' theorem
- All quantities are divided up into _data_ (i.e. things which have been observed) and _parameters_ (i.e. things which haven't been observed)
- We use Bayes' interpretation of the theorem to get the _posterior probability distribution_, the probability of the unobserved given the observed
- Used now in almost all areas of statistical application (finance, medicine, environmetrics, gambling, etc, etc)

## Why is this relevant to SIMMs?

- Easy to specify Bayesian models hierarchically in layers so that the data depend on some parameters, which then depend on further parameters, and so on. This allows us to create richer statistical models which will better match reality
- Almost all the modern Stable Isotope Mixing Models (SIMMs) use Bayesian statistics
- MixSIR, SIAR, MixSIAR, simmr, IsotopeR, ...

## What is Bayes' theorem?

Bayes' theorem can be written in words as:

$$\mbox{posterior is proportional to likelihood times prior}$$
... or ...
$$\mbox{posterior} \propto \mbox{likelihood} \times \mbox{prior}$$
  
Each of the three terms _posterior_, _likelihood_, and _prior_ are _probability distributions_ (pdfs).

In a Bayesian model, every item of interest is either data (which we will write as $x$) or parameters (which we will write as $\theta$). Often the parameters are divided up into those of interest, and other _nuisance parameters_

## Bayes' theorem in more detail

Bayes' equation is usually written mathematically as:
$$p(\theta|x) \propto p(x|\theta) \times p(\theta)$$
or, more fully:
$$p(\theta|x) = \frac{p(x|\theta) \times p(\theta)}{p(x)}$$

- The _posterior_ is the probability of the parameters given the data
- The _likelihood_ is the probability of observing the data given the parameters (unknowns)
- The _prior_ represents external knowledge about the parameters


## A very simple linear regression example

Suppose you had some data that looked like this:
```{r, echo=FALSE}
dat = read.csv('../data/earnings.csv')
with(dat, plot(height_cm, log(earn)))
```

## What you are used to doing

\tiny
```{r}
model = lm(log(earn) ~ height_cm, data = dat)
summary(model)
```
\normalsize

## What you will now get instead

```{r, echo=FALSE, results='hide', message=FALSE}
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
stan_code = '
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real intercept;
  real slope;
  real<lower=0> residual_sd;
} 
model {
  y ~ normal(intercept + slope * x, residual_sd);
}
'
stan_run = stan(data = list(N=nrow(dat), 
                            y = log(dat$earn),
                            x = dat$height_cm),
                model_code = stan_code)
```
\tiny
```{r}
print(stan_run)
```
\normalsize 

## Using prior information

- The Bayesian model in the previous slide divided up everything into _parameters_ (the intercept, slope and residual standard deviation), and data (the height and log(earnings) values)
- The software in the background created a posterior probability distribution of the parameters given the data
- The model I fitted used no _prior information_. However, if we had done a previous experiment that suggested the intercept should be around 9 with standard deviation 0.5 we can put this in the model

## A model with prior information

```{r, echo=FALSE, results='hide', message=FALSE}
stan_code_2 = '
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real intercept;
  real slope;
  real<lower=0> residual_sd;
} model {
  intercept ~ normal(9, 0.5);
  y ~ normal(intercept + slope * x, residual_sd);
}
'
stan_run_2 = stan(data = list(N=nrow(dat), 
                            y = log(dat$earn),
                            x = dat$height_cm),
                model_code = stan_code_2)
```
\tiny
```{r}
print(stan_run_2)
```
\normalsize 

## An early example of a Bayesian model

- To create the Bayesian version of this model I used the following Stan code:

\scriptsize
```{r, eval = FALSE}
stan_code = '
data {
  int N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real intercept;
  real slope;
  real<lower=0> residual_sd;
} 
model {
  // Likelihood
  y ~ normal(intercept + slope * x, residual_sd);
  // Priors
  intercept ~ normal(0, 100);
  slope ~ normal(0, 100);
  residual_sd ~ uniform(0, 100);
}
'
```
\normalsize

## How do I specify the prior distribution?

There are several choices when it comes to specifying prior distributions:

- _Informative_, when there is information from a previous study, or other good external source, e.g $\theta \sim N(2.3,0.5^2)$
- _Vague_, when there is only weak information, perhaps as to the likely range of the parameter e.g. $\theta \sim U(0,100)$
- _Flat_, when there is no information at all about a parameter (very rare). In JAGS, write `theta ~ dflat()`

Choosing the prior and choosing the likelihood are very similar problems

## Choosing likelihoods and priors {.smaller}

When creating Bayesian models it's helpful to know a lot of probability distributions. The ones we will use most are:

Distribution  | Range         | Useful for:
------------- | ------------- | --------------------------
Normal, $N(\mu,\sigma^2)$        | $(-\infty,\infty$)  | A good default choice
Uniform, $U(a,b)$  | $(a,b)$  | Vague priors when we only know the range of the parameter
Binomial, $Bin(k,\theta)$ | $[0,k]$ | Count or binary data restricted to have an upper value
Poisson, $Po(\lambda)$ | $[0,\infty)$ | Count data with no upper limit
Gamma, $Ga(\alpha,\beta)$ | $(0,\infty)$ | Continuous data with a lower bound of zero
Multivariate Normal, $MVN(\mu,\Sigma)$ | $(-\infty,\infty$) | Multivariate unbounded data with correlation between parameters/observations
Dirichlet, $Dir(\alpha_1, \ldots, \alpha_k)$ | $(0, 1)$ | Multivariate proportions which must sum to 1


## Creating the posterior distribution

- In the very simple example, I was able to calculate the posterior distribution in just a couple of lines of `R` code 
- When we have lots of parameters, and complicated prior distributions, we have to resort to _simulation_
- This means that we obtain _samples_ from the posterior distribution rather than creating the probability distribution directly
- JAGS uses Markov chain Monte Carlo (MCMC) to create these samples. We will talk about this a bit more in later lectures/discussion

## Summarising the posterior distribution

- Because we obtain samples from the posterior distribution, we can create any quantity we like from them
- e.g. we can obtain the mean or standard deviation simply from combining the samples together
- We can create quantiles e.g. 50% for the median
- We can create a Bayesian _credible interval_ (CI) by calculating lower and upper quantiles
- When the posterior distribution is messy (e.g. multi-modal) we can use a _highest posterior density_ (HPD) region

## Example: {.smaller}

```{r,include=FALSE}
library(rjags)
modelstring ='
model {
  x ~ dnorm(theta,1/pow(0.8,2))
  theta ~ dnorm(2.3,1/pow(0.5,2))
}
'
data=list(x=3.1)
model=jags.model(textConnection(modelstring), data=data)
output=coda.samples(model=model,variable.names=c("theta"), n.iter=10000)
```

From the earlier simple example. First 5 posterior samples
```{r}
output[[1]][1:5]
```
The mean and standard deviation: 
```{r}
c(mean(output[[1]]),sd(output[[1]]))
```
A 95% credible interval
```{r}
quantile(output[[1]],probs=c(0.025,0.975))
```

## Why is this better?

The Bayesian approach has numerous advantages:

- It's easier to build complex models and to analyse the parameters you want directly
- We automatically obtain the best parameter estimates and their uncertainty from the posterior samples
- It allows us to get away from (terrible) null hypothesis testing and $p$-values

## Some further reading

- The Bayesian bible: Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). _Bayesian Data Analysis_, Third Edition. CRC Press.
- The MCMC bible: Brooks, S., Gelman, A., Jones, G., & Meng, X. (2011). _Handbook of Markov Chain Monte Carlo_. CRC Press.
- Something simpler: McCarthy, M. A. (2007). _Bayesian Methods for Ecology_. Cambridge University Press.

## Summary

- Bayesian statistical models involve a likelihood and a prior. These both need to be carefully chosen. From these we create a posterior distribution
- The likelihood represents the information about the data generating process, the prior represents information about the unknown parameters
- We usually create and analyse samples from the posterior probability distribution of the unknowns (the parameters) given the knowns (the data)
- From the posterior distribution we can create means, medians, standard deviations, credible intervals, etc

